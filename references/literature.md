This directionary includes the references used in the project.

### 1. Mattia & Festa, 2025 – *Sonify Collective Human Intelligence: A Biometric Data Approach to Real-time Sound Design in HCI*
- **Study focus:** A biometric data–driven interactive installation (SCHI) that transforms real-time physiological signals into sound textures for a multisensory experience. :contentReference[oaicite:0]{index=0}
- **Relevance to Chirp:** Shows how biometric/physiological signals can be mapped to sound, suggesting a possible way to sonify parrot emotional data for owners.  
- **Limitations/Advantages:** Good inspiration for emotion sonification ideas; not about animal emotion specifically and not an AI model.

### 2. Tsiourti et al., 2019 – *Multimodal Integration of Emotional Signals from Voice, Body, and Context*  
- **Study focus:** Empirical study on how humans recognize robot emotions using voice and body cues, especially when cues are congruent vs incongruent. :contentReference[oaicite:1]{index=1}
- **Relevance to Chirp:** Highlights the importance of multimodal emotion signals—voice + behavior—for recognizing affective states, which is useful if you combine parrot vocalization with behavior.  
- **Limitations/Advantages:** Advantage is demonstration of multimodal emotion perception; limitation is human–robot interaction context, not animals.

### 3. Huang & Situ, 2025 – *Beyond Discrete Categories: Multi-Task Valence-Arousal Modeling for Pet Vocalization Analysis*
- **Study focus:** A **continuous Valence-Arousal (VA)** model for pet vocalizations using automatic labeling and multi-task learning on 42,553 samples. :contentReference[oaicite:2]{index=2}
- **Relevance to Chirp:** Directly relevant: provides a continuous emotion representation method that could be adapted for parrot vocalization interpretation.  
- **Limitations/Advantages:** Advantage is a fine‑grained emotion model with strong performance; limitation is that it’s trained on common pets (e.g., dogs), so parrot adaptation is needed.

### 4. Piseddu et al., 2024 – *What we (don’t) know about parrot welfare*  
- **Study focus:** Systematic review of scientific welfare indicators for parrots (behavior, feathers, vocalizations, etc.).  
- **Relevance to Chirp:** Gives parrot‑specific behavioral/emotional indicators you can use to label or interpret emotional states.  
- **Limitations/Advantages:** Advantage is solid, species‑specific indicators; limitation is lack of AI modeling techniques.

### 5. *BirdNET* – Bird species identification from audio by BirdNET Team.  
[GitHub link](https://github.com/birdnet-team/birdnet)
- **Relevance to Chirp:** Gives inspiration on the model to be used: Convolutional Neural Network (CNN).

### 6. Robinson et al., 2024 – *NatureLM‑audio: an Audio‑Language Foundation Model for Bioacoustics*
- **Study focus:** Introduces **NatureLM‑audio**, the first audio-language foundation model for bioacoustics. It is designed to classify, detect, and caption animal vocalizations, supporting zero-shot recognition of unseen species and tasks.  
- **Link:** [arXiv](https://arxiv.org/abs/2411.07186)
- **Relevance to Chirp:** Audio-language joint learning could be adapted to **parrot vocalizations + descriptive emotion labels**, allowing the model to predict emotions in a zero-shot or low-data scenario.
- **Limitations:** Designed for species and behavior classification, not emotion; fine-tuning with parrot-specific emotion labels is necessary. It requires substantial paired text-audio datasets; generating accurate emotion labels for parrots may be challenging. 
